# ğŸ“˜ Script 5: `research_quantization.py`

## Overview

This script implements **quantization** - the technique of reducing the numerical precision of model weights and activations. Instead of using 32-bit floating-point numbers, we use smaller representations like 16-bit, 8-bit, or even 4-bit numbers.

**Why quantization is the "last mile" of compression:**
- Provides 2-8Ã— size reduction with minimal code changes
- Often the easiest compression technique to apply
- Works well combined with KD and pruning for maximum compression
- Critical for deploying to mobile devices, CPUs, and edge hardware

---

## The Big Picture: What Quantization Does

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUANTIZATION: REDUCING NUMERICAL PRECISION                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ ORIGINAL WEIGHT (FP32 - 32 bits):                                          â”‚
â”‚                                                                             â”‚
â”‚   0.123456789012345678901234567890123                                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 32 bits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                                                             â”‚
â”‚   Range: Â±3.4 Ã— 10Â³â¸                                                        â”‚
â”‚   Precision: ~7 decimal digits                                             â”‚
â”‚   Memory: 4 bytes per weight                                               â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ QUANTIZED TO FP16 (16 bits):                                               â”‚
â”‚                                                                             â”‚
â”‚   0.1235                                                                    â”‚
â”‚   â””â”€â”€ 16 bits â”€â”€â”˜                                                           â”‚
â”‚                                                                             â”‚
â”‚   Range: Â±65,504                                                            â”‚
â”‚   Precision: ~3-4 decimal digits                                           â”‚
â”‚   Memory: 2 bytes per weight (2Ã— compression)                              â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ QUANTIZED TO INT8 (8 bits):                                                â”‚
â”‚                                                                             â”‚
â”‚   31 (representing ~0.12 after scaling)                                    â”‚
â”‚   â”” 8 bits â”˜                                                                â”‚
â”‚                                                                             â”‚
â”‚   Range: -128 to 127 (mapped to original weight range)                     â”‚
â”‚   Memory: 1 byte per weight (4Ã— compression)                               â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ QUANTIZED TO INT4 (4 bits):                                                â”‚
â”‚                                                                             â”‚
â”‚   7 (representing ~0.12 after scaling)                                     â”‚
â”‚   â””4bâ”˜                                                                      â”‚
â”‚                                                                             â”‚
â”‚   Range: -8 to 7 (or 0 to 15 unsigned)                                     â”‚
â”‚   Memory: 0.5 bytes per weight (8Ã— compression!)                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 1: Imports and Setup

```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import torch.nn as nn
import torch.quantization as quant
from torch.quantization import (
    quantize_dynamic,
    prepare,
    convert,
    get_default_qconfig,
    QConfig,
    default_observer,
    default_weight_observer
)
import numpy as np
from typing import Dict, Optional, Tuple, List, Union
from tqdm import tqdm
import time
import copy
```

### Key Imports Explained:

| Import | Purpose |
|--------|---------|
| `quantize_dynamic` | Dynamic quantization (easiest method) |
| `prepare`, `convert` | Static quantization workflow |
| `QConfig` | Configuration for quantization observers |
| `default_observer` | Tracks activation statistics |
| `default_weight_observer` | Tracks weight statistics |

### Understanding PyTorch Quantization Architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PYTORCH QUANTIZATION SYSTEM                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ OBSERVERS: Track statistics to determine quantization parameters           â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ MinMaxObserver                                                      â”‚  â”‚
â”‚   â”‚   Tracks: min and max values                                        â”‚  â”‚
â”‚   â”‚   Use: Simple, works for most cases                                 â”‚  â”‚
â”‚   â”‚   Formula: scale = (max - min) / 255                                â”‚  â”‚
â”‚   â”‚            zero_point = -min / scale                                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ MovingAverageMinMaxObserver                                         â”‚  â”‚
â”‚   â”‚   Tracks: Exponential moving average of min/max                     â”‚  â”‚
â”‚   â”‚   Use: When values change over time (training)                      â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ HistogramObserver                                                   â”‚  â”‚
â”‚   â”‚   Tracks: Full histogram of values                                  â”‚  â”‚
â”‚   â”‚   Use: Most accurate, but slower                                    â”‚  â”‚
â”‚   â”‚   Finds optimal scale by minimizing quantization error              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚ QCONFIG: Combines observers for weights and activations                    â”‚
â”‚                                                                             â”‚
â”‚   qconfig = QConfig(                                                        â”‚
â”‚       activation=default_observer,    # How to observe activations         â”‚
â”‚       weight=default_weight_observer  # How to observe weights             â”‚
â”‚   )                                                                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 2: The Quantization Manager Class

```python
class QuantizationManager:
    """
    Manages all quantization operations for a model.
    
    Supports multiple quantization methods:
        - dynamic: Easiest, weights quantized, activations at runtime
        - static: Both pre-quantized, needs calibration
        - qat: Quantization-aware training
        - fp16: Half precision (GPU friendly)
        - int4: 4-bit quantization (maximum compression)
    """
    
    def __init__(
        self,
        model: nn.Module,
        method: str = 'dynamic',
        dtype: str = 'int8'
    ):
        """
        Args:
            model: Model to quantize
            method: Quantization method ('dynamic', 'static', 'qat', 'fp16', 'int4')
            dtype: Target data type ('int8', 'int4', 'fp16')
        """
        self.model = model
        self.method = method
        self.dtype = dtype
        self.original_size = self._get_model_size(model)
        self.quantized_model = None
```

### Getting Model Size:

```python
def _get_model_size(self, model: nn.Module) -> float:
    """
    Calculate model size in megabytes.
    
    Two approaches:
    1. Count parameters Ã— bytes per parameter
    2. Save model and check file size (more accurate for quantized)
    """
    # Method 1: Parameter counting
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    
    total_size_mb = (param_size + buffer_size) / (1024 ** 2)
    
    return total_size_mb
```

### Understanding `element_size()`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DATA TYPES AND THEIR SIZES                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Data Type      â”‚ Bits â”‚ Bytes â”‚ element_size() â”‚ Range                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ torch.float32  â”‚  32  â”‚   4   â”‚       4        â”‚ Â±3.4 Ã— 10Â³â¸              â”‚
â”‚ torch.float16  â”‚  16  â”‚   2   â”‚       2        â”‚ Â±65,504                   â”‚
â”‚ torch.bfloat16 â”‚  16  â”‚   2   â”‚       2        â”‚ Â±3.4 Ã— 10Â³â¸ (less precise)â”‚
â”‚ torch.int8     â”‚   8  â”‚   1   â”‚       1        â”‚ -128 to 127               â”‚
â”‚ torch.qint8    â”‚   8  â”‚   1   â”‚       1        â”‚ -128 to 127 (quantized)   â”‚
â”‚ torch.quint8   â”‚   8  â”‚   1   â”‚       1        â”‚ 0 to 255 (unsigned)       â”‚
â”‚                                                                             â”‚
â”‚ Example calculation:                                                        â”‚
â”‚   BanglaBERT: 110M parameters Ã— 4 bytes = 440 MB                           â”‚
â”‚   After INT8: 110M parameters Ã— 1 byte = 110 MB (4Ã— smaller!)              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 3: Dynamic Quantization (The Easiest Method)

Dynamic quantization is the simplest approach. Weights are quantized ahead of time, but activations are quantized dynamically during inference.

```python
def apply_dynamic_quantization(self) -> nn.Module:
    """
    Apply dynamic quantization to the model.
    
    HOW IT WORKS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    1. Weights are quantized ONCE when you call this function
    2. Activations are quantized ON-THE-FLY during each forward pass
    3. No calibration data needed!
    
    WHY "DYNAMIC"?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    The quantization parameters for activations are computed dynamically
    based on the actual values seen during inference. This means:
    - No need for representative calibration data
    - Works well when activation ranges vary
    - Slightly slower than static (computes stats at runtime)
    
    LIMITATIONS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Only works on CPU (PyTorch limitation)
    - Cannot run on GPU after quantization
    - Good for: server deployment, batch processing
    """
    print(f"\nğŸ“‰ Applying dynamic quantization (INT8)...")
    print(f"   âš ï¸  Note: Dynamic quantization runs on CPU only")
    
    # Move model to CPU first
    model_cpu = copy.deepcopy(self.model).cpu()
    model_cpu.eval()
    
    # Apply dynamic quantization
    # Only quantize Linear layers (where most computation happens)
    quantized_model = quantize_dynamic(
        model_cpu,
        {nn.Linear},          # Which layer types to quantize
        dtype=torch.qint8     # Target data type
    )
    
    self.quantized_model = quantized_model
    
    # Report compression
    new_size = self._get_model_size(quantized_model)
    compression = self.original_size / new_size
    print(f"   Size: {self.original_size:.1f} MB â†’ {new_size:.1f} MB")
    print(f"   Compression: {compression:.2f}Ã—")
    
    return quantized_model
```

### Visual: How Dynamic Quantization Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DYNAMIC QUANTIZATION WORKFLOW                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ STEP 1: QUANTIZE WEIGHTS (done once, at model load time)                   â”‚
â”‚                                                                             â”‚
â”‚   Original weight (FP32):  [0.123, -0.456, 0.789, ...]                     â”‚
â”‚                                                                             â”‚
â”‚   Compute scale and zero_point:                                            â”‚
â”‚     min_val = -0.456, max_val = 0.789                                      â”‚
â”‚     scale = (0.789 - (-0.456)) / 255 = 0.00488                            â”‚
â”‚     zero_point = round(-(-0.456) / 0.00488) = 93                          â”‚
â”‚                                                                             â”‚
â”‚   Quantize:                                                                 â”‚
â”‚     q_weight = round(weight / scale) + zero_point                          â”‚
â”‚     q_weight = [118, 0, 255, ...]  (INT8 values)                          â”‚
â”‚                                                                             â”‚
â”‚   Store: q_weight (INT8) + scale (FP32) + zero_point (INT32)              â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ STEP 2: INFERENCE (happens for each input)                                 â”‚
â”‚                                                                             â”‚
â”‚   Input activation (FP32):  [1.5, -0.3, 2.1, ...]                         â”‚
â”‚                                                                             â”‚
â”‚   Dynamically quantize activation:                                         â”‚
â”‚     Compute min/max of THIS batch                                          â”‚
â”‚     Compute scale and zero_point                                           â”‚
â”‚     q_activation = [...]  (INT8)                                           â”‚
â”‚                                                                             â”‚
â”‚   Matrix multiply in INT8:                                                  â”‚
â”‚     q_output = q_activation @ q_weight                                     â”‚
â”‚     (This is the fast part! INT8 ops are 2-4Ã— faster than FP32)           â”‚
â”‚                                                                             â”‚
â”‚   Dequantize output back to FP32:                                          â”‚
â”‚     output = (q_output - zero_point) Ã— scale                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Modification | Effect | When to Use |
|--------------|--------|-------------|
| `{nn.Linear, nn.Conv2d}` | Quantize Conv layers too | If model has convolutions |
| `dtype=torch.quint8` | Unsigned quantization | When values are always positive |

---

## Section 4: Static Quantization (Better Accuracy)

Static quantization pre-computes quantization parameters for both weights AND activations using calibration data. This gives better accuracy but requires representative data.

```python
def prepare_static_quantization(self) -> nn.Module:
    """
    Prepare model for static quantization.
    
    This inserts "observer" modules that will track statistics
    during calibration. The model can still run in FP32 mode.
    
    WORKFLOW:
    1. prepare_static_quantization()  â† You are here
    2. calibrate()                    â† Run representative data
    3. convert_static_quantization()  â† Finalize quantization
    """
    print(f"\nğŸ“‰ Preparing for static quantization...")
    
    # Move to CPU (required for PyTorch quantization)
    model_cpu = copy.deepcopy(self.model).cpu()
    model_cpu.eval()
    
    # Set quantization configuration
    # This tells PyTorch how to observe and quantize
    model_cpu.qconfig = get_default_qconfig('fbgemm')
    # 'fbgemm' is optimized for server CPUs (x86)
    # 'qnnpack' is optimized for mobile CPUs (ARM)
    
    # Prepare the model
    # This inserts observer modules throughout the model
    prepared_model = prepare(model_cpu, inplace=False)
    
    self.prepared_model = prepared_model
    print(f"   âœ“ Model prepared with observers")
    print(f"   Next step: Run calibration data through the model")
    
    return prepared_model
```

### Calibration Step:

```python
def calibrate(
    self,
    dataloader,
    device: str = 'cpu',
    num_batches: int = 100
) -> None:
    """
    Run calibration data through the prepared model.
    
    WHY CALIBRATE?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Static quantization needs to know the typical range of activations
    BEFORE inference. We run representative data and observers track:
    - Minimum activation value seen
    - Maximum activation value seen
    - (Optionally) Full histogram of values
    
    These statistics are used to compute scale and zero_point.
    
    HOW MUCH DATA?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Too little: Might miss extreme values, causing clipping
    - Too much: Diminishing returns, wastes time
    - Rule of thumb: 100-1000 samples is usually sufficient
    """
    print(f"   Calibrating with {num_batches} batches...")
    
    self.prepared_model.eval()
    
    with torch.no_grad():
        for i, batch in enumerate(tqdm(dataloader, total=num_batches)):
            if i >= num_batches:
                break
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # Forward pass - observers record statistics
            _ = self.prepared_model(input_ids, attention_mask)
    
    print(f"   âœ“ Calibration complete")
```

### Converting to Quantized Model:

```python
def convert_static_quantization(self) -> nn.Module:
    """
    Convert the prepared+calibrated model to a quantized model.
    
    This is where the actual quantization happens:
    1. Observers are removed
    2. Quantization parameters are computed from observed stats
    3. Weights are quantized
    4. Quantize/Dequantize operations are inserted
    """
    print(f"   Converting to quantized model...")
    
    # Convert the model
    quantized_model = convert(self.prepared_model, inplace=False)
    
    self.quantized_model = quantized_model
    
    # Report results
    new_size = self._get_model_size(quantized_model)
    compression = self.original_size / new_size
    print(f"   Size: {self.original_size:.1f} MB â†’ {new_size:.1f} MB")
    print(f"   Compression: {compression:.2f}Ã—")
    
    return quantized_model
```

### Visual: Static vs Dynamic Quantization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STATIC VS DYNAMIC QUANTIZATION                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ DYNAMIC QUANTIZATION:                                                       â”‚
â”‚                                                                             â”‚
â”‚   Preparation:  None needed                                                â”‚
â”‚   Calibration:  None needed                                                â”‚
â”‚                                                                             â”‚
â”‚   Inference (each batch):                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Input â†’ [Compute activation stats] â†’ [Quantize activation] â†’        â”‚  â”‚
â”‚   â”‚         [INT8 matmul with pre-quantized weights] â†’ [Dequantize] â†’ Outâ”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                    â†‘                                                        â”‚
â”‚                    Extra computation at runtime                             â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ STATIC QUANTIZATION:                                                        â”‚
â”‚                                                                             â”‚
â”‚   Preparation:  Run calibration data once                                  â”‚
â”‚   Calibration:  Observers track min/max of all activations                 â”‚
â”‚                                                                             â”‚
â”‚   Inference (each batch):                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ Input â†’ [Quantize with PRE-COMPUTED params] â†’                        â”‚  â”‚
â”‚   â”‚         [INT8 matmul] â†’ [Dequantize] â†’ Output                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚              â†‘                                                              â”‚
â”‚              No runtime stat computation (faster!)                         â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ COMPARISON:                                                                 â”‚
â”‚                                                                             â”‚
â”‚                    â”‚ Dynamic        â”‚ Static                               â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚   Calibration      â”‚ Not needed     â”‚ Required (100+ samples)              â”‚
â”‚   Accuracy         â”‚ Good           â”‚ Better (more accurate params)        â”‚
â”‚   Speed            â”‚ Fast           â”‚ Faster (no runtime stats)            â”‚
â”‚   Flexibility      â”‚ High           â”‚ Lower (fixed activation range)       â”‚
â”‚   Best for         â”‚ Variable data  â”‚ Consistent data distribution         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 5: Quantization-Aware Training (QAT)

QAT simulates quantization during training, allowing the model to learn to be robust to quantization errors.

```python
def prepare_qat(self) -> nn.Module:
    """
    Prepare model for Quantization-Aware Training.
    
    WHY QAT?
    â”€â”€â”€â”€â”€â”€â”€â”€
    Normal quantization: Train in FP32 â†’ Quantize â†’ Hope it works
    QAT: Train WITH simulated quantization â†’ Model learns to handle it
    
    HOW IT WORKS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    During forward pass:
        1. Weights are fake-quantized (quantize then dequantize)
        2. This introduces quantization noise
        3. Model learns to be robust to this noise
    
    During backward pass:
        1. Gradients flow through as if no quantization
        2. This is called "Straight-Through Estimator" (STE)
    
    Result: Better accuracy after final quantization!
    
    TRADEOFF:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Requires full training (expensive!)
    - Best accuracy of all methods
    - Use when accuracy is critical
    """
    print(f"\nğŸ“‰ Preparing for Quantization-Aware Training...")
    
    # Move to CPU for quantization setup, but can train on GPU
    model_copy = copy.deepcopy(self.model)
    model_copy.train()  # Must be in training mode
    
    # Set QAT configuration
    model_copy.qconfig = get_default_qconfig('fbgemm')
    
    # Prepare for QAT
    # This inserts FakeQuantize modules
    prepared_model = prepare(model_copy, inplace=False)
    
    self.qat_model = prepared_model
    print(f"   âœ“ Model prepared for QAT")
    print(f"   Next: Train this model, then convert with convert_qat()")
    
    return prepared_model
```

### Visual: How QAT Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUANTIZATION-AWARE TRAINING (QAT)                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ NORMAL TRAINING (FP32):                                                     â”‚
â”‚                                                                             â”‚
â”‚   Weight: 0.12345678                                                        â”‚
â”‚      â†“                                                                      â”‚
â”‚   Multiply with input                                                       â”‚
â”‚      â†“                                                                      â”‚
â”‚   Output: Perfect precision                                                 â”‚
â”‚                                                                             â”‚
â”‚   After training â†’ Quantize â†’ Weight becomes 0.12 â†’ ACCURACY DROP!         â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ QAT (Simulated Quantization):                                              â”‚
â”‚                                                                             â”‚
â”‚   Weight: 0.12345678                                                        â”‚
â”‚      â†“                                                                      â”‚
â”‚   FAKE QUANTIZE: round to INT8, then back to FP32                          â”‚
â”‚      â†“                                                                      â”‚
â”‚   Fake quantized weight: 0.12 (in FP32 format, but limited precision)      â”‚
â”‚      â†“                                                                      â”‚
â”‚   Multiply with input (using imprecise weight)                             â”‚
â”‚      â†“                                                                      â”‚
â”‚   Output: Includes quantization noise!                                      â”‚
â”‚      â†“                                                                      â”‚
â”‚   Loss computed with noisy output                                          â”‚
â”‚      â†“                                                                      â”‚
â”‚   Model learns to be robust to noise                                        â”‚
â”‚                                                                             â”‚
â”‚   After training â†’ Quantize â†’ Weight becomes 0.12 â†’ MINIMAL DROP!          â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ THE STRAIGHT-THROUGH ESTIMATOR (STE):                                       â”‚
â”‚                                                                             â”‚
â”‚   Problem: round() has zero gradient almost everywhere!                     â”‚
â”‚            d/dx round(x) = 0 (except at integers where undefined)          â”‚
â”‚            Gradient descent would not work.                                 â”‚
â”‚                                                                             â”‚
â”‚   Solution: Pretend round() is the identity function during backward:       â”‚
â”‚                                                                             â”‚
â”‚   Forward:  x â†’ round(x/s)*s â†’ y   (includes rounding)                     â”‚
â”‚   Backward: dy/dx = 1              (ignores rounding)                       â”‚
â”‚                                                                             â”‚
â”‚   This lets gradients flow through quantization operations!                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### QAT Training Loop:

```python
def train_qat(
    self,
    train_loader,
    val_loader,
    optimizer,
    num_epochs: int = 5,
    device: str = 'cpu'
) -> nn.Module:
    """
    Train the QAT-prepared model.
    
    The training loop is almost identical to normal training!
    The FakeQuantize modules handle the quantization simulation.
    """
    print(f"\nğŸ”§ Training with QAT for {num_epochs} epochs...")
    
    model = self.qat_model.to(device)
    loss_fn = nn.BCEWithLogitsLoss()
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for batch in tqdm(train_loader, desc=f"QAT Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            # Forward pass with fake quantization
            outputs = model(input_ids, attention_mask)
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            
            loss = loss_fn(logits, labels)
            loss.backward()  # Gradients flow through FakeQuantize via STE
            optimizer.step()
            
            total_loss += loss.item()
        
        print(f"   Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}")
    
    # Convert to final quantized model
    model.eval()
    model.cpu()
    quantized_model = convert(model, inplace=False)
    
    self.quantized_model = quantized_model
    return quantized_model
```

---

## Section 6: FP16 Quantization (GPU-Friendly)

Unlike INT8 which only works on CPU in PyTorch, FP16 works on GPU and is much simpler.

```python
def apply_fp16_quantization(self, device: str = 'cuda') -> nn.Module:
    """
    Convert model to half precision (FP16).
    
    WHY FP16?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Works on GPU! (Unlike INT8 in standard PyTorch)
    - 2Ã— memory reduction
    - Often FASTER on modern GPUs (tensor cores)
    - Minimal accuracy loss (~0.1% typically)
    
    HOW IT WORKS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Simply convert all FP32 parameters to FP16.
    No calibration, no training, just type conversion.
    
    CAUTION:
    â”€â”€â”€â”€â”€â”€â”€â”€
    - Some operations may need FP32 for stability (loss computation)
    - Very small/large values may overflow/underflow
    - Use torch.cuda.amp for automatic mixed precision if concerned
    """
    print(f"\nğŸ“‰ Applying FP16 quantization...")
    print(f"   âœ“ Works on GPU!")
    
    # Simply convert to half precision
    model_fp16 = copy.deepcopy(self.model).half().to(device)
    
    self.quantized_model = model_fp16
    
    # Report compression (always 2Ã— for FP16)
    print(f"   Size: {self.original_size:.1f} MB â†’ {self.original_size/2:.1f} MB")
    print(f"   Compression: 2.00Ã—")
    
    return model_fp16
```

### FP32 vs FP16 vs BF16:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FLOATING POINT FORMATS                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ FP32 (Single Precision):                                                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚Sign â”‚    Exponent (8)    â”‚           Mantissa (23)                   â”‚   â”‚
â”‚ â”‚  1  â”‚    8 bits          â”‚           23 bits                         â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚   Range: Â±3.4 Ã— 10Â³â¸                                                        â”‚
â”‚   Precision: ~7 decimal digits                                             â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ FP16 (Half Precision):                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚ â”‚Sign â”‚ Exp (5)   â”‚   Mantissa (10)   â”‚                                    â”‚
â”‚ â”‚  1  â”‚  5 bits   â”‚    10 bits        â”‚                                    â”‚
â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚   Range: Â±65,504 (MUCH smaller!)                                           â”‚
â”‚   Precision: ~3 decimal digits                                             â”‚
â”‚   Good for: Most neural network computations                               â”‚
â”‚   Bad for: Loss computation, gradient accumulation                         â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ BF16 (Brain Float):                                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚ â”‚Sign â”‚    Exponent (8)    â”‚ Mant (7)  â”‚                                   â”‚
â”‚ â”‚  1  â”‚    8 bits          â”‚  7 bits   â”‚                                   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚   Range: Â±3.4 Ã— 10Â³â¸ (same as FP32!)                                       â”‚
â”‚   Precision: ~2 decimal digits (less than FP16)                            â”‚
â”‚   Good for: Training (same range as FP32, less overflow risk)              â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ PRACTICAL ADVICE:                                                           â”‚
â”‚                                                                             â”‚
â”‚   Training:   Use BF16 or mixed precision (FP16 + FP32 where needed)       â”‚
â”‚   Inference:  Use FP16 (best speed/accuracy tradeoff)                      â”‚
â”‚   Edge/Mobile: Use INT8 (maximum compression)                               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 7: INT4 Quantization (Maximum Compression)

INT4 provides 8Ã— compression but requires special libraries (bitsandbytes) and careful implementation.

```python
def apply_int4_quantization(self, device: str = 'cuda') -> nn.Module:
    """
    Apply 4-bit quantization using bitsandbytes library.
    
    WHY INT4?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - 8Ã— compression (vs 4Ã— for INT8)
    - Still maintains reasonable accuracy
    - Used by QLoRA for efficient LLM fine-tuning
    
    HOW IT WORKS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Uses NF4 (Normalized Float 4-bit) from bitsandbytes:
    1. Weights are normalized to have zero mean and unit variance
    2. Quantized to 4 bits using a learned code book
    3. Double quantization: Even the scaling factors are quantized!
    
    REQUIREMENTS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - pip install bitsandbytes
    - CUDA GPU required
    """
    print(f"\nğŸ“‰ Applying INT4 quantization...")
    
    try:
        import bitsandbytes as bnb
        from transformers import BitsAndBytesConfig
    except ImportError:
        print("   âŒ bitsandbytes not installed!")
        print("   Run: pip install bitsandbytes")
        print("   Falling back to INT8...")
        return self.apply_dynamic_quantization()
    
    # Create quantization config
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,  # Compute in FP16 for speed
        bnb_4bit_use_double_quant=True,         # Quantize the quantization constants!
        bnb_4bit_quant_type="nf4"               # Normalized Float 4-bit
    )
```

### Converting Linear Layers to 4-bit:

```python
    def replace_linear_with_4bit(model):
        """
        Replace all Linear layers with 4-bit quantized versions.
        """
        for name, module in model.named_children():
            if isinstance(module, nn.Linear):
                # Create 4-bit linear layer
                new_layer = bnb.nn.Linear4bit(
                    module.in_features,
                    module.out_features,
                    bias=module.bias is not None,
                    compute_dtype=torch.float16,
                    quant_type="nf4"
                )
                
                # Quantize and copy weights
                new_layer.weight = bnb.nn.Params4bit(
                    module.weight.data,
                    requires_grad=False,
                    quant_type="nf4"
                )
                
                if module.bias is not None:
                    new_layer.bias = nn.Parameter(module.bias.data)
                
                setattr(model, name, new_layer)
            else:
                # Recursively process child modules
                replace_linear_with_4bit(module)
        
        return model
    
    # Apply 4-bit quantization
    model_4bit = replace_linear_with_4bit(copy.deepcopy(self.model))
    model_4bit = model_4bit.to(device)
    
    self.quantized_model = model_4bit
    
    # Report compression
    new_size = self.original_size / 8  # Approximate
    print(f"   Size: {self.original_size:.1f} MB â†’ ~{new_size:.1f} MB")
    print(f"   Compression: ~8.00Ã—")
    
    return model_4bit
```

### Visual: NF4 Quantization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NF4 (NORMALIZED FLOAT 4-BIT) QUANTIZATION                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ PROBLEM WITH UNIFORM INT4:                                                  â”‚
â”‚                                                                             â”‚
â”‚   Weight distribution is typically GAUSSIAN, not uniform:                  â”‚
â”‚                                                                             â”‚
â”‚         â”‚        â–„â–„                                                         â”‚
â”‚   Count â”‚       â–ˆâ–ˆâ–ˆâ–ˆ                                                        â”‚
â”‚         â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                       â”‚
â”‚         â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                      â”‚
â”‚         â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                     â”‚
â”‚         â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                    â”‚
â”‚         â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                   â”‚
â”‚         â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚           -1.0       0       +1.0                                           â”‚
â”‚                                                                             â”‚
â”‚   Uniform INT4: 16 evenly spaced values across range                       â”‚
â”‚   Problem: Most weights are near 0, wasting precision on extremes!         â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ NF4 SOLUTION:                                                               â”‚
â”‚                                                                             â”‚
â”‚   Use quantization levels that match the distribution:                     â”‚
â”‚                                                                             â”‚
â”‚   NF4 code book (16 values, not evenly spaced):                            â”‚
â”‚   [-1.0, -0.7, -0.5, -0.4, -0.3, -0.2, -0.1, 0,                           â”‚
â”‚     0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0]                                  â”‚
â”‚                                                                             â”‚
â”‚   More levels near 0 (where most weights are) = better accuracy!           â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ DOUBLE QUANTIZATION:                                                        â”‚
â”‚                                                                             â”‚
â”‚   Normal quantization stores:                                              â”‚
â”‚     - Quantized weights (INT4)                                             â”‚
â”‚     - Scale factors (FP32) â† Still 32 bits!                                â”‚
â”‚                                                                             â”‚
â”‚   Double quantization:                                                      â”‚
â”‚     - Quantized weights (INT4)                                             â”‚
â”‚     - Quantized scale factors (INT8) â† Even smaller!                       â”‚
â”‚     - Second-level scale (FP32, shared across many weights)                â”‚
â”‚                                                                             â”‚
â”‚   Extra compression at minimal accuracy cost!                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 8: Benchmarking and Comparison

```python
def benchmark_inference_speed(
    self,
    model: nn.Module,
    dataloader,
    device: str,
    num_batches: int = 50
) -> Dict[str, float]:
    """
    Measure inference speed and latency.
    
    Returns:
        - latency_mean: Average time per batch (ms)
        - latency_p95: 95th percentile latency (ms)
        - throughput: Samples per second
    """
    model.eval()
    latencies = []
    total_samples = 0
    
    # Warm-up (first few runs are often slower)
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= 3:
                break
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            _ = model(input_ids, attention_mask)
    
    # Actual benchmarking
    with torch.no_grad():
        for i, batch in enumerate(dataloader):
            if i >= num_batches:
                break
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            # Synchronize GPU before timing
            if device == 'cuda':
                torch.cuda.synchronize()
            
            start_time = time.time()
            
            _ = model(input_ids, attention_mask)
            
            if device == 'cuda':
                torch.cuda.synchronize()
            
            end_time = time.time()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            total_samples += len(input_ids)
    
    total_time = sum(latencies) / 1000  # Total time in seconds
    
    return {
        'latency_mean': np.mean(latencies),
        'latency_std': np.std(latencies),
        'latency_p50': np.percentile(latencies, 50),
        'latency_p95': np.percentile(latencies, 95),
        'latency_p99': np.percentile(latencies, 99),
        'throughput': total_samples / total_time
    }
```

### Compare Sizes:

```python
def compare_sizes(self) -> Dict[str, float]:
    """
    Compare original and quantized model sizes.
    """
    if self.quantized_model is None:
        return {'error': 'No quantized model available'}
    
    original_size = self.original_size
    quantized_size = self._get_model_size(self.quantized_model)
    
    return {
        'original_size_mb': original_size,
        'quantized_size_mb': quantized_size,
        'compression_ratio': original_size / quantized_size,
        'size_reduction_percent': (1 - quantized_size / original_size) * 100
    }
```

---

## Section 9: The Main Quantize Function

```python
def quantize_model(
    model: nn.Module,
    method: str = 'dynamic',
    dataloader = None,
    device: str = 'cpu',
    num_calibration_batches: int = 100,
    qat_epochs: int = 3,
    optimizer = None
) -> nn.Module:
    """
    Main entry point for quantizing a model.
    
    This is what research_main.py calls.
    """
    manager = QuantizationManager(model, method=method)
    
    if method == 'dynamic':
        return manager.apply_dynamic_quantization()
    
    elif method == 'static':
        manager.prepare_static_quantization()
        manager.calibrate(dataloader, device, num_calibration_batches)
        return manager.convert_static_quantization()
    
    elif method == 'qat':
        if optimizer is None:
            raise ValueError("QAT requires an optimizer")
        manager.prepare_qat()
        return manager.train_qat(dataloader, None, optimizer, qat_epochs, device)
    
    elif method == 'fp16':
        return manager.apply_fp16_quantization(device)
    
    elif method == 'int4':
        return manager.apply_int4_quantization(device)
    
    else:
        raise ValueError(f"Unknown quantization method: {method}")
```

---

## Complete Method Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUANTIZATION METHOD COMPARISON                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Method   â”‚ Bits â”‚ Compress â”‚ Device â”‚ Calibration â”‚ Accuracy â”‚ Speed       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ None     â”‚  32  â”‚    1Ã—    â”‚ GPU    â”‚     -       â”‚ Baseline â”‚ 1.0Ã—        â”‚
â”‚ FP16     â”‚  16  â”‚    2Ã—    â”‚ GPU âœ“  â”‚    None     â”‚ ~0% loss â”‚ 1.0-1.5Ã—    â”‚
â”‚ Dynamic  â”‚   8  â”‚    4Ã—    â”‚ CPU âš   â”‚    None     â”‚ 1-2% lossâ”‚ 1.5-2Ã—      â”‚
â”‚ Static   â”‚   8  â”‚    4Ã—    â”‚ CPU âš   â”‚   Required  â”‚ 0.5-1% â†“ â”‚ 2-3Ã—        â”‚
â”‚ QAT      â”‚   8  â”‚    4Ã—    â”‚ CPU âš   â”‚  Training   â”‚ 0-0.5% â†“ â”‚ 2-3Ã—        â”‚
â”‚ INT4     â”‚   4  â”‚    8Ã—    â”‚ GPU âœ“  â”‚   Required  â”‚ 2-4% lossâ”‚ 1.5-2Ã—      â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ DECISION FLOWCHART:                                                         â”‚
â”‚                                                                             â”‚
â”‚   Need GPU inference?                                                       â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â”œâ”€â”€ YES â†’ Need maximum compression?                                   â”‚
â”‚       â”‚          â”‚                                                          â”‚
â”‚       â”‚          â”œâ”€â”€ YES â†’ INT4 (8Ã— compression)                           â”‚
â”‚       â”‚          â”‚                                                          â”‚
â”‚       â”‚          â””â”€â”€ NO â†’ FP16 (2Ã— compression, minimal loss)              â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â””â”€â”€ NO (CPU is fine) â†’ Accuracy critical?                            â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â”œâ”€â”€ YES â†’ Have training time?                  â”‚
â”‚                              â”‚          â”‚                                   â”‚
â”‚                              â”‚          â”œâ”€â”€ YES â†’ QAT                       â”‚
â”‚                              â”‚          â”‚                                   â”‚
â”‚                              â”‚          â””â”€â”€ NO â†’ Static                     â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â””â”€â”€ NO â†’ Dynamic (simplest)                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 10: ONNX Export (Alternative Quantization Path)

Sometimes PyTorch quantization is limiting. ONNX provides an alternative path that works on more platforms.

```python
def export_to_onnx(
    model: nn.Module,
    save_path: str,
    input_shape: Tuple[int, int] = (1, 128),
    quantize: bool = True
) -> str:
    """
    Export model to ONNX format with optional quantization.
    
    WHY ONNX?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - Platform independent (runs on Windows, Linux, Mac, mobile)
    - Supports more quantization options
    - Often faster than PyTorch inference
    - Required for some deployment targets (Edge devices, TensorRT)
    
    ONNX QUANTIZATION OPTIONS:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    - QOperator: Quantize operators (like PyTorch static)
    - QDQ: Insert Quantize/Dequantize nodes (more flexible)
    """
    import torch.onnx
    
    model.eval()
    model.cpu()
    
    # Create dummy input
    dummy_input = (
        torch.zeros(input_shape, dtype=torch.long),  # input_ids
        torch.ones(input_shape, dtype=torch.long)    # attention_mask
    )
    
    # Export to ONNX
    onnx_path = save_path if save_path.endswith('.onnx') else f"{save_path}.onnx"
    
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        input_names=['input_ids', 'attention_mask'],
        output_names=['logits'],
        dynamic_axes={
            'input_ids': {0: 'batch', 1: 'sequence'},
            'attention_mask': {0: 'batch', 1: 'sequence'},
            'logits': {0: 'batch'}
        },
        opset_version=14
    )
    
    print(f"   âœ“ Exported to: {onnx_path}")
    
    # Optionally quantize the ONNX model
    if quantize:
        try:
            from onnxruntime.quantization import quantize_dynamic as onnx_quantize
            
            quantized_path = onnx_path.replace('.onnx', '_quantized.onnx')
            onnx_quantize(onnx_path, quantized_path)
            
            print(f"   âœ“ Quantized ONNX saved to: {quantized_path}")
            return quantized_path
            
        except ImportError:
            print("   âš ï¸  onnxruntime not installed, skipping ONNX quantization")
    
    return onnx_path
```

---

## Summary: What You Can Modify in This Script

| Category | What to Modify | Research Question |
|----------|----------------|-------------------|
| **Method** | dynamic, static, qat, fp16, int4 | Which method gives best accuracy/compression? |
| **Calibration** | Number of batches | How much calibration data is needed? |
| **QAT** | Number of epochs | How much training improves accuracy? |
| **Layers** | Which layers to quantize | Are some layers more sensitive? |
| **Backend** | fbgemm vs qnnpack | Best backend for your hardware? |
| **Observer** | MinMax vs Histogram | Which observer works better? |

---

## Experiments You Can Run

```bash
# Experiment 1: Compare all quantization methods
for method in dynamic static fp16; do
    python research_main.py --pipeline quant_only \
        --quant_method $method \
        --output_dir results/quant_$method
done

# Experiment 2: Calibration size study (for static quantization)
for batches in 10 50 100 500; do
    python research_main.py --pipeline quant_only \
        --quant_method static \
        --quant_calibration_batches $batches \
        --output_dir results/calib_$batches
done

# Experiment 3: Combined compression (KD + Quantization)
python research_main.py --pipeline kd_quant \
    --kd_method logit \
    --quant_method fp16 \
    --output_dir results/kd_fp16

# Experiment 4: Full compression pipeline
python research_main.py --pipeline kd_prune_quant \
    --kd_method multi_level \
    --prune_method wanda \
    --prune_sparsity 0.5 \
    --quant_method fp16 \
    --output_dir results/full_compression
```

---

## Key Takeaways

1. **FP16 is the safest choice** - works on GPU, minimal accuracy loss, simple to apply
2. **INT8 is for CPU deployment** - use dynamic if no calibration data, static otherwise
3. **INT4 is for maximum compression** - requires bitsandbytes, best for very large models
4. **QAT is for best accuracy** - requires full training, use when every 0.1% matters
5. **Always benchmark** - speed gains vary by hardware and model

---

## Practice Exercise

Before moving to the next script:

1. **Calculate the theoretical compression**: If a model has 110M FP32 parameters, how big is it in MB? After INT8? After INT4?
2. **Think about trade-offs**: Why might FP16 be faster than FP32 on modern GPUs?
3. **Consider the deployment**: If you're deploying to a phone, which method would you choose and why?

---

**Ready for the next script? The next one is `research_evaluation.py` which implements comprehensive metrics calculation and comparison across all compression stages.**

Would you like me to continue?