# ğŸ“˜ Script 4: `research_pruning.py`

## Overview

This script implements **pruning** - the technique of removing unimportant weights from a neural network. Think of it like trimming a tree: you remove branches that don't contribute much, making the tree lighter while keeping its essential structure.

**Why pruning matters:**
- Reduces model size by 30-70%
- Can speed up inference (with proper hardware/libraries)
- Often combined with KD and quantization for maximum compression
- Research shows neural networks are heavily over-parameterized

---

## The Big Picture: What Pruning Does

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ THE LOTTERY TICKET HYPOTHESIS (Frankle & Carlin, 2019)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Key insight: Large neural networks contain smaller subnetworks              â”‚
â”‚ ("winning tickets") that can achieve similar accuracy alone.               â”‚
â”‚                                                                             â”‚
â”‚ Original Network (110M parameters):                                        â”‚
â”‚                                                                             â”‚
â”‚   â—â”â”â”â—â”â”â”â—â”â”â”â—â”â”â”â—                                                        â”‚
â”‚   â”ƒ â•² â”ƒ â•± â”ƒ â•² â”ƒ â•± â”ƒ                                                        â”‚
â”‚   â—â”â”â”â—â”â”â”â—â”â”â”â—â”â”â”â—     All connections present                            â”‚
â”‚   â”ƒ â•± â”ƒ â•² â”ƒ â•± â”ƒ â•² â”ƒ     Many are redundant or near-zero                    â”‚
â”‚   â—â”â”â”â—â”â”â”â—â”â”â”â—â”â”â”â—                                                        â”‚
â”‚                                                                             â”‚
â”‚ After 50% Pruning (55M parameters):                                        â”‚
â”‚                                                                             â”‚
â”‚   â—â”â”â”â—   â—â”â”â”â—   â—                                                        â”‚
â”‚   â”ƒ   â”ƒ â•±     â”ƒ â•± â”ƒ                                                        â”‚
â”‚   â—   â—â”â”â”â—â”â”â”â—   â—     Removed weak connections                           â”‚
â”‚   â”ƒ â•±     â”ƒ       â”ƒ     Kept important pathways                            â”‚
â”‚   â—â”â”â”â—â”â”â”â—   â—â”â”â”â—                                                        â”‚
â”‚                                                                             â”‚
â”‚ Result: Similar accuracy with half the parameters!                         â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 1: Imports and Constants

```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
from tqdm import tqdm
from collections import defaultdict
import copy
```

### Key Import: `torch.nn.utils.prune`

PyTorch provides a built-in pruning module. Understanding its design helps you understand all pruning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HOW PYTORCH PRUNING WORKS                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ STEP 1: Before pruning                                                     â”‚
â”‚                                                                             â”‚
â”‚   layer.weight = tensor([0.5, -0.02, 0.3, 0.01, -0.4])                    â”‚
â”‚                                                                             â”‚
â”‚ STEP 2: Apply pruning mask                                                 â”‚
â”‚                                                                             â”‚
â”‚   prune.l1_unstructured(layer, 'weight', amount=0.4)                       â”‚
â”‚                                                                             â”‚
â”‚   What happens internally:                                                 â”‚
â”‚   - Original weight is saved as 'weight_orig'                              â”‚
â”‚   - A mask is created: mask = [1, 0, 1, 0, 1]  (0 = pruned)               â”‚
â”‚   - 'weight' becomes a property: weight = weight_orig * mask               â”‚
â”‚                                                                             â”‚
â”‚   layer.weight_orig = tensor([0.5, -0.02, 0.3, 0.01, -0.4])               â”‚
â”‚   layer.weight_mask = tensor([1,   0,     1,   0,    1   ])               â”‚
â”‚   layer.weight      = tensor([0.5, 0,     0.3, 0,   -0.4])  # computed    â”‚
â”‚                                                                             â”‚
â”‚ STEP 3: Make permanent (optional)                                          â”‚
â”‚                                                                             â”‚
â”‚   prune.remove(layer, 'weight')                                            â”‚
â”‚                                                                             â”‚
â”‚   Now:                                                                      â”‚
â”‚   layer.weight = tensor([0.5, 0, 0.3, 0, -0.4])  # actual zeros           â”‚
â”‚   (weight_orig and weight_mask are removed)                                â”‚
â”‚                                                                             â”‚
â”‚ WHY THIS DESIGN?                                                            â”‚
â”‚   - Masks allow pruning to be reversible                                   â”‚
â”‚   - Multiple pruning rounds can be combined                                â”‚
â”‚   - Gradual pruning can adjust masks during training                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 2: Base Pruning Manager Class

```python
class PruningManager:
    """
    Base class for all pruning operations.
    
    Provides:
        - Layer identification (which layers to prune)
        - Sparsity tracking (how many zeros)
        - Common utilities
    """
    
    def __init__(
        self,
        model: nn.Module,
        target_sparsity: float = 0.5,
        prune_layers: str = 'all',
        global_pruning: bool = True
    ):
        """
        Args:
            model: Model to prune
            target_sparsity: Fraction of weights to remove (0.5 = 50%)
            prune_layers: Which layers to prune ('all', 'attention', 'ffn', 'encoder')
            global_pruning: If True, prune globally; if False, prune each layer independently
        """
        self.model = model
        self.target_sparsity = target_sparsity
        self.prune_layers = prune_layers
        self.global_pruning = global_pruning
        
        # Find layers to prune
        self.prunable_layers = self._identify_prunable_layers()
```

### Understanding `prune_layers` Options:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHICH LAYERS TO PRUNE?                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ TRANSFORMER ARCHITECTURE:                                                   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ ENCODER BLOCK (Ã—12 for BERT-base)                                   â”‚  â”‚
â”‚   â”‚                                                                     â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚   â”‚   â”‚ ATTENTION LAYER                                             â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Query:  Linear(768 â†’ 768)   â† prune_layers='attention'   â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Key:    Linear(768 â†’ 768)                                 â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Value:  Linear(768 â†’ 768)                                 â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Output: Linear(768 â†’ 768)                                 â”‚  â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚   â”‚                           â†“                                        â”‚  â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚   â”‚   â”‚ FFN LAYER (Feed-Forward Network)                            â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Intermediate: Linear(768 â†’ 3072)  â† prune_layers='ffn'   â”‚  â”‚  â”‚
â”‚   â”‚   â”‚   Output:       Linear(3072 â†’ 768)                          â”‚  â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚   â”‚                                                                     â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   prune_layers='all'       â†’ Prune everything (attention + FFN)            â”‚
â”‚   prune_layers='attention' â†’ Prune only Q, K, V, O projections             â”‚
â”‚   prune_layers='ffn'       â†’ Prune only feed-forward layers                â”‚
â”‚   prune_layers='encoder'   â†’ Prune encoder only, not classifier            â”‚
â”‚                                                                             â”‚
â”‚ RESEARCH INSIGHT:                                                           â”‚
â”‚   FFN layers have 2/3 of transformer parameters!                           â”‚
â”‚   Pruning FFN aggressively often works well.                               â”‚
â”‚   Pruning attention too much hurts model's ability to "pay attention"      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layer Identification Method:

```python
def _identify_prunable_layers(self) -> List[Tuple[nn.Module, str]]:
    """
    Find all layers that can be pruned based on prune_layers setting.
    
    Returns list of (module, parameter_name) tuples.
    """
    prunable = []
    
    for name, module in self.model.named_modules():
        # Only prune Linear layers (have weight matrices)
        if not isinstance(module, nn.Linear):
            continue
        
        # Filter based on prune_layers setting
        if self.prune_layers == 'all':
            prunable.append((module, 'weight'))
            
        elif self.prune_layers == 'attention':
            # Attention layers have 'query', 'key', 'value', or 'attention' in name
            if any(x in name.lower() for x in ['query', 'key', 'value', 'attention']):
                prunable.append((module, 'weight'))
                
        elif self.prune_layers == 'ffn':
            # FFN layers have 'intermediate' or 'output' (but not attention output)
            if 'intermediate' in name.lower() or ('output' in name.lower() and 'attention' not in name.lower()):
                prunable.append((module, 'weight'))
                
        elif self.prune_layers == 'encoder':
            # Everything except classifier
            if 'classifier' not in name.lower():
                prunable.append((module, 'weight'))
    
    print(f"   Found {len(prunable)} prunable layers")
    return prunable
```

### Global vs Local Pruning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GLOBAL VS LOCAL PRUNING                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ LOCAL PRUNING (prune each layer independently):                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚
â”‚                                                                             â”‚
â”‚   Layer 1 weights: [0.5, 0.4, 0.3, 0.2, 0.1]   â†’ Remove 2 smallest        â”‚
â”‚   After pruning:   [0.5, 0.4, 0.3, 0,   0  ]   â†’ 40% sparse               â”‚
â”‚                                                                             â”‚
â”‚   Layer 2 weights: [0.9, 0.8, 0.7, 0.6, 0.5]   â†’ Remove 2 smallest        â”‚
â”‚   After pruning:   [0.9, 0.8, 0.7, 0,   0  ]   â†’ 40% sparse               â”‚
â”‚                                                                             â”‚
â”‚   Problem: Layer 2's smallest (0.5, 0.6) are LARGER than Layer 1's        â”‚
â”‚            largest (0.3)! We removed important weights!                    â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ GLOBAL PRUNING (consider all layers together):                             â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚                                                                             â”‚
â”‚   All weights: [0.5, 0.4, 0.3, 0.2, 0.1, 0.9, 0.8, 0.7, 0.6, 0.5]         â”‚
â”‚   Sorted:      [0.1, 0.2, 0.3, 0.4, 0.5, 0.5, 0.6, 0.7, 0.8, 0.9]         â”‚
â”‚                                                                             â”‚
â”‚   Remove 40% (4 smallest): 0.1, 0.2, 0.3, 0.4                              â”‚
â”‚                                                                             â”‚
â”‚   Layer 1: [0.5, 0,   0,   0,   0  ]  â†’ 80% sparse (had small weights)    â”‚
â”‚   Layer 2: [0.9, 0.8, 0.7, 0.6, 0.5]  â†’ 0% sparse (had large weights)     â”‚
â”‚                                                                             â”‚
â”‚   Better! Globally, we removed the truly unimportant weights.              â”‚
â”‚                                                                             â”‚
â”‚ RECOMMENDATION: Use global_pruning=True (default)                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 3: Magnitude Pruning (Simplest Method)

```python
def apply_magnitude_pruning(self):
    """
    Prune weights with smallest absolute magnitude.
    
    The simplest and most common pruning method.
    Intuition: Small weights contribute little to output.
    
    Formula:
        importance(w) = |w|
        prune if |w| < threshold
    """
    print(f"\nâœ‚ï¸  Applying magnitude pruning (sparsity={self.target_sparsity*100:.0f}%)")
    
    if self.global_pruning:
        # Gather all weights into a single list
        parameters_to_prune = [
            (module, name) for module, name in self.prunable_layers
        ]
        
        # Apply global unstructured pruning
        prune.global_unstructured(
            parameters_to_prune,
            pruning_method=prune.L1Unstructured,
            amount=self.target_sparsity
        )
    else:
        # Prune each layer independently
        for module, name in self.prunable_layers:
            prune.l1_unstructured(module, name, amount=self.target_sparsity)
    
    sparsity_info = self.get_sparsity()
    print(f"   Achieved sparsity: {sparsity_info['overall']*100:.2f}%")
```

### How L1 Unstructured Pruning Works:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L1 (MAGNITUDE) PRUNING ALGORITHM                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ STEP 1: Collect all weights                                                â”‚
â”‚                                                                             â”‚
â”‚   weights = [0.8, -0.05, 0.3, 0.01, -0.6, 0.02, 0.9, -0.03, 0.15, -0.7]   â”‚
â”‚                                                                             â”‚
â”‚ STEP 2: Take absolute values                                               â”‚
â”‚                                                                             â”‚
â”‚   |weights| = [0.8, 0.05, 0.3, 0.01, 0.6, 0.02, 0.9, 0.03, 0.15, 0.7]     â”‚
â”‚                                                                             â”‚
â”‚ STEP 3: Find threshold for target sparsity (50%)                           â”‚
â”‚                                                                             â”‚
â”‚   Sorted: [0.01, 0.02, 0.03, 0.05, 0.15, 0.3, 0.6, 0.7, 0.8, 0.9]         â”‚
â”‚                                    â†‘                                        â”‚
â”‚                              50th percentile                                â”‚
â”‚   threshold = 0.15                                                          â”‚
â”‚                                                                             â”‚
â”‚ STEP 4: Create mask                                                        â”‚
â”‚                                                                             â”‚
â”‚   mask = |weight| > threshold                                              â”‚
â”‚   mask = [1, 0, 1, 0, 1, 0, 1, 0, 1, 1]                                    â”‚
â”‚                                                                             â”‚
â”‚ STEP 5: Apply mask                                                         â”‚
â”‚                                                                             â”‚
â”‚   pruned_weights = weights * mask                                          â”‚
â”‚   pruned_weights = [0.8, 0, 0.3, 0, -0.6, 0, 0.9, 0, 0.15, -0.7]          â”‚
â”‚                                                                             â”‚
â”‚   50% of weights are now zero!                                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Modification | Effect | When to Use |
|--------------|--------|-------------|
| `target_sparsity=0.3` | Conservative, 30% removed | When accuracy is critical |
| `target_sparsity=0.7` | Aggressive, 70% removed | When compression is critical |
| `global_pruning=False` | Per-layer pruning | Ensure uniform sparsity |
| `prune_layers='ffn'` | Only prune FFN | Preserve attention capability |

---

## Section 4: Gradual Pruning (Better Accuracy)

Gradual pruning is more sophisticated. Instead of removing all weights at once, it slowly increases sparsity during training, allowing the model to adapt.

```python
class GradualPruner:
    """
    Gradually increase sparsity during training.
    
    WHY GRADUAL?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    One-shot pruning: Remove 50% instantly â†’ model breaks â†’ hard to recover
    Gradual pruning:  Remove 5%, adapt, remove 5%, adapt... â†’ model survives
    
    Research shows gradual pruning achieves 2-5% better accuracy
    at the same sparsity level compared to one-shot.
    """
    
    def __init__(
        self,
        model: nn.Module,
        target_sparsity: float = 0.5,
        start_epoch: int = 0,
        end_epoch: int = 10,
        schedule: str = 'cubic',
        prune_frequency: int = 100,
        prune_layers: str = 'all'
    ):
        """
        Args:
            model: Model to prune
            target_sparsity: Final sparsity to achieve
            start_epoch: When to start pruning
            end_epoch: When to reach target sparsity
            schedule: How to increase sparsity ('linear', 'cubic', 'exponential')
            prune_frequency: Prune every N training steps
            prune_layers: Which layers to prune
        """
        self.model = model
        self.target_sparsity = target_sparsity
        self.start_epoch = start_epoch
        self.end_epoch = end_epoch
        self.schedule = schedule
        self.prune_frequency = prune_frequency
        self.current_sparsity = 0.0
        
        # Initialize pruning masks (all ones = nothing pruned yet)
        self.prunable_layers = self._identify_prunable_layers()
        self._initialize_masks()
```

### Understanding Pruning Schedules:

```python
def _compute_sparsity_for_step(self, current_step: int, total_steps: int) -> float:
    """
    Compute target sparsity for the current training step.
    
    Different schedules provide different pruning trajectories.
    """
    # Compute progress (0 to 1)
    progress = min(1.0, current_step / total_steps)
    
    if self.schedule == 'linear':
        # Linear: Constant pruning rate
        # sparsity = target Ã— progress
        sparsity = self.target_sparsity * progress
        
    elif self.schedule == 'cubic':
        # Cubic: Slow start, fast middle, slow end (RECOMMENDED)
        # sparsity = target Ã— (1 - (1 - progress)Â³)
        sparsity = self.target_sparsity * (1 - (1 - progress) ** 3)
        
    elif self.schedule == 'exponential':
        # Exponential: Slow start, increasingly fast
        # sparsity = target Ã— (1 - exp(-5 Ã— progress))
        sparsity = self.target_sparsity * (1 - np.exp(-5 * progress))
    
    return sparsity
```

### Visual Comparison of Schedules:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRUNING SCHEDULE COMPARISON                                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Sparsity (%)                                                               â”‚
â”‚     â”‚                                                                       â”‚
â”‚ 50% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â—â— â”€â”€â”€ Target        â”‚
â”‚     â”‚                                         â—â—â—â—                          â”‚
â”‚     â”‚                                     â—â—â—â—     â—â—â—â—                     â”‚
â”‚ 40% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—                  â”‚
â”‚     â”‚                              â—â—â—                   â—â—                 â”‚
â”‚     â”‚                          â—â—â—â—                       â—                 â”‚
â”‚ 30% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—               â”‚
â”‚     â”‚                    â—â—â—                                â—               â”‚
â”‚     â”‚                 â—â—â—                                                   â”‚
â”‚ 20% â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚     â”‚           â—â—â—                                                         â”‚
â”‚     â”‚        â—â—â—                                                            â”‚
â”‚ 10% â”œâ”€â”€â”€â”€â”€â—â—â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚     â”‚   â—â—                                                                  â”‚
â”‚     â”‚ â—â—                                                                    â”‚
â”‚  0% â”œâ—â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚
â”‚       0    1    2    3    4    5    6    7    8    9    10  Epoch          â”‚
â”‚                                                                             â”‚
â”‚     â—â—â—â—â— LINEAR:       Constant rate (simplest)                           â”‚
â”‚     â”€â”€â”€â”€â”€ CUBIC:        Slow-fast-slow (best for accuracy)                 â”‚
â”‚     Â·Â·Â·Â·Â· EXPONENTIAL:  Slow start, fast end                               â”‚
â”‚                                                                             â”‚
â”‚ WHY CUBIC IS BEST:                                                          â”‚
â”‚   - Slow start: Model has time to identify important weights               â”‚
â”‚   - Fast middle: Remove weights efficiently                                 â”‚
â”‚   - Slow end: Fine-tune remaining structure                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Critical Step Method:

```python
def step(self, current_step: int, total_steps: int):
    """
    Called every training step. May or may not prune depending on frequency.
    
    This is the heart of gradual pruning!
    """
    # Only prune every prune_frequency steps
    if current_step % self.prune_frequency != 0:
        return
    
    # Compute target sparsity for this step
    target = self._compute_sparsity_for_step(current_step, total_steps)
    
    # If target hasn't increased, nothing to do
    if target <= self.current_sparsity:
        return
    
    # CRITICAL: Compute INCREMENTAL amount to prune
    # This is where many implementations go wrong!
    incremental_sparsity = self._compute_incremental_sparsity(target)
    
    # Apply incremental pruning
    self._apply_incremental_pruning(incremental_sparsity)
    
    self.current_sparsity = target
```

### The Incremental Sparsity Bug (And Fix):

```python
def _compute_incremental_sparsity(self, new_target: float) -> float:
    """
    Compute how much MORE to prune to reach new target.
    
    CRITICAL BUG in naive implementations:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    WRONG approach:
        Current: 30% sparse
        Target: 40% sparse
        Naive: Prune 10% of ORIGINAL weights
        
        But 30% are already zero! So we're pruning 10% of 70% = 7%
        Result: 30% + 7% = 37% (not 40%!)
        
    CORRECT approach:
        Current: 30% sparse (70% remain)
        Target: 40% sparse (60% should remain)
        Need to remove: (70% - 60%) / 70% = 14.3% of REMAINING weights
        
    Formula:
        incremental = (target - current) / (1 - current)
    """
    if self.current_sparsity >= 1.0:
        return 0.0
    
    # Correct incremental calculation
    incremental = (new_target - self.current_sparsity) / (1 - self.current_sparsity)
    
    return min(incremental, 1.0)  # Cap at 100%
```

### Visual Explanation of the Bug:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHY INCREMENTAL CALCULATION MATTERS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ SCENARIO: Go from 30% to 40% sparsity                                      â”‚
â”‚                                                                             â”‚
â”‚ Original weights: [w1, w2, w3, w4, w5, w6, w7, w8, w9, w10]                â”‚
â”‚                                                                             â”‚
â”‚ After 30% pruning:                                                          â”‚
â”‚   [w1, 0, 0, w4, w5, w6, 0, w8, w9, w10]                                   â”‚
â”‚   3 zeros out of 10 = 30% sparse                                           â”‚
â”‚   7 non-zero weights remain                                                â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ WRONG: Prune 10% of original (1 weight)                                    â”‚
â”‚   Remove 1 from [w1, w4, w5, w6, w8, w9, w10]                              â”‚
â”‚   Result: 4 zeros â†’ 40% âœ“ (got lucky with 10 weights!)                    â”‚
â”‚                                                                             â”‚
â”‚   But with 1000 weights:                                                   â”‚
â”‚   30% sparse = 300 zeros, 700 remain                                       â”‚
â”‚   Prune 10% of 1000 = 100 weights                                          â”‚
â”‚   Result: 300 + 100 = 400 zeros = 40% âœ“                                   â”‚
â”‚                                                                             â”‚
â”‚   Wait, that worked? Let's try 50% â†’ 60%:                                  â”‚
â”‚   50% sparse = 500 zeros, 500 remain                                       â”‚
â”‚   Prune 10% of 1000 = 100 weights                                          â”‚
â”‚   Result: 500 + 100 = 600 zeros = 60% âœ“                                   â”‚
â”‚                                                                             â”‚
â”‚   Now 60% â†’ 70%:                                                           â”‚
â”‚   60% sparse = 600 zeros, 400 remain                                       â”‚
â”‚   Prune 10% of 1000 = 100 weights                                          â”‚
â”‚   Result: 600 + 100 = 700 zeros = 70% âœ“                                   â”‚
â”‚                                                                             â”‚
â”‚   Now 70% â†’ 80%:                                                           â”‚
â”‚   70% sparse = 700 zeros, 300 remain                                       â”‚
â”‚   Prune 10% of 1000 = 100 weights                                          â”‚
â”‚   BUT we only have 300 non-zero! Can't prune 100!                         â”‚
â”‚   âŒ BREAKS when remaining < increment                                     â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ CORRECT: Prune fraction of REMAINING weights                               â”‚
â”‚                                                                             â”‚
â”‚   30% â†’ 40%:                                                               â”‚
â”‚   Remaining = 70%                                                          â”‚
â”‚   Need to reach: 60% remaining                                             â”‚
â”‚   Prune: (70% - 60%) / 70% = 14.3% of remaining                           â”‚
â”‚                                                                             â”‚
â”‚   50% â†’ 60%:                                                               â”‚
â”‚   Remaining = 50%                                                          â”‚
â”‚   Need to reach: 40% remaining                                             â”‚
â”‚   Prune: (50% - 40%) / 50% = 20% of remaining                             â”‚
â”‚                                                                             â”‚
â”‚   70% â†’ 80%:                                                               â”‚
â”‚   Remaining = 30%                                                          â”‚
â”‚   Need to reach: 20% remaining                                             â”‚
â”‚   Prune: (30% - 20%) / 30% = 33.3% of remaining                           â”‚
â”‚   âœ“ Always possible!                                                       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 5: Wanda Pruning (State-of-the-Art)

Wanda (Weights AND Activations) is a 2023 method that considers not just weight magnitude but also how much each weight is actually used during inference.

```python
class WandaPruner:
    """
    Wanda Pruning: Pruning by Weights AND Activations
    
    Paper: "A Simple and Effective Pruning Approach for Large Language Models"
           (Sun et al., 2023)
    
    KEY INSIGHT:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    A small weight might still be important if it's multiplied by large activations!
    
    Example:
        weight = 0.1 (small)
        activation = 10.0 (large)
        contribution = 0.1 Ã— 10.0 = 1.0 (significant!)
        
    Magnitude pruning would remove this weight. Wanda keeps it.
    
    FORMULA:
        importance(w) = |w| Ã— ||activation||
        
    where ||activation|| is the L2 norm of activations that multiply with w.
    """
    
    def __init__(
        self,
        model: nn.Module,
        target_sparsity: float = 0.5,
        prune_layers: str = 'all'
    ):
        self.model = model
        self.target_sparsity = target_sparsity
        self.prune_layers = prune_layers
        
        # Will store activation statistics
        self.activation_norms = {}
        self.hooks = []
```

### Collecting Activation Statistics:

```python
def collect_activations(
    self,
    dataloader,
    device: str,
    num_samples: int = 512
) -> None:
    """
    Run calibration data through model to collect activation statistics.
    
    This is what makes Wanda "data-aware" - it sees how the model
    actually uses each weight on real data.
    """
    print(f"   Collecting activations from {num_samples} samples...")
    
    self.model.eval()
    self.activation_norms = defaultdict(list)
    
    # Register hooks to capture activations
    def make_hook(name):
        def hook(module, input, output):
            # input[0] is the activation entering this layer
            # Shape: (batch, seq_len, hidden_size)
            activation = input[0].detach()
            
            # Compute L2 norm across sequence dimension
            # Shape: (batch, hidden_size)
            norm = torch.norm(activation, p=2, dim=1)
            
            # Average across batch
            # Shape: (hidden_size,)
            mean_norm = norm.mean(dim=0)
            
            self.activation_norms[name].append(mean_norm.cpu())
        
        return hook
    
    # Register hooks on all prunable layers
    for name, module in self.model.named_modules():
        if isinstance(module, nn.Linear) and self._should_prune(name):
            hook = module.register_forward_hook(make_hook(name))
            self.hooks.append(hook)
    
    # Run calibration data through model
    samples_seen = 0
    with torch.no_grad():
        for batch in dataloader:
            if samples_seen >= num_samples:
                break
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            _ = self.model(input_ids, attention_mask)
            samples_seen += len(input_ids)
    
    # Remove hooks
    for hook in self.hooks:
        hook.remove()
    self.hooks = []
    
    # Average activation norms across all batches
    for name in self.activation_norms:
        norms = torch.stack(self.activation_norms[name])
        self.activation_norms[name] = norms.mean(dim=0)
    
    print(f"   âœ“ Collected activations for {len(self.activation_norms)} layers")
```

### Applying Wanda Pruning:

```python
def apply_wanda_pruning(self):
    """
    Apply Wanda pruning using collected activation statistics.
    """
    print(f"\nâœ‚ï¸  Applying Wanda pruning (sparsity={self.target_sparsity*100:.0f}%)")
    
    for name, module in self.model.named_modules():
        if not isinstance(module, nn.Linear):
            continue
        if name not in self.activation_norms:
            continue
        
        weight = module.weight.data  # Shape: (out_features, in_features)
        activation_norm = self.activation_norms[name].to(weight.device)
        
        # Compute Wanda importance scores
        # importance = |weight| Ã— activation_norm
        # Broadcasting: (out, in) Ã— (in,) = (out, in)
        importance = weight.abs() * activation_norm.unsqueeze(0)
        
        # Find threshold for target sparsity
        threshold = torch.quantile(importance.flatten(), self.target_sparsity)
        
        # Create mask
        mask = (importance > threshold).float()
        
        # Apply mask
        module.weight.data *= mask
    
    print(f"   âœ“ Wanda pruning complete")
```

### Visual: Magnitude vs Wanda:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MAGNITUDE VS WANDA PRUNING                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Example layer with 8 weights:                                              â”‚
â”‚                                                                             â”‚
â”‚   Weight:     [0.8,  0.05, 0.3,  0.02, 0.6,  0.01, 0.9,  0.1 ]            â”‚
â”‚   Activation: [1.0,  5.0,  1.0,  1.0,  1.0,  1.0,  1.0,  8.0 ]            â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ MAGNITUDE PRUNING (|weight| only):                                         â”‚
â”‚                                                                             â”‚
â”‚   Importance: [0.8,  0.05, 0.3,  0.02, 0.6,  0.01, 0.9,  0.1 ]            â”‚
â”‚   Sorted:     [0.01, 0.02, 0.05, 0.1,  0.3,  0.6,  0.8,  0.9 ]            â”‚
â”‚                                                                             â”‚
â”‚   50% pruning removes: 0.01, 0.02, 0.05, 0.1                               â”‚
â”‚                                                                             â”‚
â”‚   Result:     [0.8,  0,    0.3,  0,    0.6,  0,    0.9,  0   ]            â”‚
â”‚               Kept   âŒ    Kept  âŒ    Kept  âŒ    Kept  âŒ               â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ WANDA PRUNING (|weight| Ã— activation):                                     â”‚
â”‚                                                                             â”‚
â”‚   Importance: [0.8,  0.25, 0.3,  0.02, 0.6,  0.01, 0.9,  0.8 ]            â”‚
â”‚                 â”‚     â”‚                                   â”‚                 â”‚
â”‚                 â”‚     â””â”€â”€ 0.05 Ã— 5.0 = 0.25 (boosted!)    â”‚                 â”‚
â”‚                 â”‚                                         â”‚                 â”‚
â”‚                 â””â”€â”€ 0.8 Ã— 1.0 = 0.8                      â””â”€â”€ 0.1 Ã— 8.0 = 0.8â”‚
â”‚                                                                             â”‚
â”‚   Sorted:     [0.01, 0.02, 0.25, 0.3,  0.6,  0.8,  0.8,  0.9 ]            â”‚
â”‚                                                                             â”‚
â”‚   50% pruning removes: 0.01, 0.02, 0.25, 0.3                               â”‚
â”‚                                                                             â”‚
â”‚   Result:     [0.8,  0,    0,    0,    0.6,  0,    0.9,  0.1 ]            â”‚
â”‚               Kept   âŒ    âŒ    âŒ    Kept  âŒ    Kept  Kept!             â”‚
â”‚                                                                             â”‚
â”‚ KEY DIFFERENCE: Wanda kept weight[7]=0.1 because activation=8.0            â”‚
â”‚                 Magnitude pruning removed it (small weight)                â”‚
â”‚                 Wanda removed weight[2]=0.3 instead (low activation)       â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What You Can Modify:

| Modification | Effect | Research Use |
|--------------|--------|--------------|
| `num_samples` | More samples = more accurate statistics | Trade off speed vs accuracy |
| Use different norm | L1 instead of L2 | Different importance measure |
| Per-output pruning | Prune within each output neuron | More uniform sparsity |

**Example - Per-Output Wanda:**
```python
# Instead of global threshold, prune within each output row
def apply_wanda_per_output(self):
    for name, module in self.model.named_modules():
        if name not in self.activation_norms:
            continue
        
        weight = module.weight.data  # (out, in)
        importance = weight.abs() * self.activation_norms[name]
        
        # Prune within each output neuron (row)
        for i in range(weight.shape[0]):
            row_importance = importance[i]
            threshold = torch.quantile(row_importance, self.target_sparsity)
            mask = (row_importance > threshold).float()
            weight[i] *= mask
```

---

## Section 6: Structured Pruning

Unlike unstructured pruning (individual weights), structured pruning removes entire neurons, attention heads, or layers. This provides **actual speedup** without special sparse libraries.

```python
class StructuredPruner:
    """
    Remove entire neurons, attention heads, or layers.
    
    WHY STRUCTURED?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Unstructured pruning creates sparse matrices:
        [0.5, 0, 0.3, 0, 0, 0.8, 0, 0.2]
        
    Standard GPUs can't skip zeros efficiently.
    You need special sparse libraries to get speedup.
    
    Structured pruning removes whole rows/columns:
        Original:  (768, 768) matrix
        Pruned:    (768, 512) matrix  â† Actually smaller!
        
    ANY hardware can benefit from smaller matrices.
    """
    
    def __init__(
        self,
        model: nn.Module,
        target_sparsity: float = 0.5,
        structure: str = 'neuron'  # 'neuron', 'head', 'layer'
    ):
        self.model = model
        self.target_sparsity = target_sparsity
        self.structure = structure
```

### Attention Head Pruning:

```python
def prune_attention_heads(self):
    """
    Remove entire attention heads based on importance.
    
    Each attention head learns different patterns:
    - Some heads focus on nearby words
    - Some heads focus on specific relationships
    - Some heads are redundant
    
    We can remove redundant heads with minimal accuracy loss.
    """
    print(f"\nâœ‚ï¸  Pruning attention heads (target: {self.target_sparsity*100:.0f}%)")
    
    # Find all attention layers
    for name, module in self.model.named_modules():
        if not self._is_attention_layer(module):
            continue
        
        num_heads = module.num_attention_heads
        head_dim = module.attention_head_size
        
        # Compute importance of each head
        # Method: L2 norm of each head's query weights
        query_weight = module.query.weight  # (hidden, hidden)
        
        head_importance = []
        for h in range(num_heads):
            start = h * head_dim
            end = (h + 1) * head_dim
            head_weights = query_weight[start:end, :]
            importance = torch.norm(head_weights, p=2)
            head_importance.append(importance.item())
        
        # Determine which heads to prune
        num_to_prune = int(num_heads * self.target_sparsity)
        heads_to_prune = np.argsort(head_importance)[:num_to_prune]
        
        # Zero out pruned heads
        for h in heads_to_prune:
            start = h * head_dim
            end = (h + 1) * head_dim
            module.query.weight.data[start:end, :] = 0
            module.key.weight.data[start:end, :] = 0
            module.value.weight.data[start:end, :] = 0
        
        print(f"   {name}: Pruned {len(heads_to_prune)}/{num_heads} heads")
```

### Visualization of Structured Pruning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STRUCTURED VS UNSTRUCTURED PRUNING                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ UNSTRUCTURED (individual weights):                                         â”‚
â”‚                                                                             â”‚
â”‚   Original matrix (4Ã—4):          After 50% unstructured:                  â”‚
â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”               â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                        â”‚
â”‚   â”‚0.5â”‚0.3â”‚0.8â”‚0.2â”‚               â”‚0.5â”‚ 0 â”‚0.8â”‚ 0 â”‚                        â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                        â”‚
â”‚   â”‚0.1â”‚0.7â”‚0.4â”‚0.9â”‚       â†’       â”‚ 0 â”‚0.7â”‚ 0 â”‚0.9â”‚                        â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                        â”‚
â”‚   â”‚0.6â”‚0.2â”‚0.5â”‚0.3â”‚               â”‚0.6â”‚ 0 â”‚0.5â”‚ 0 â”‚                        â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤                        â”‚
â”‚   â”‚0.8â”‚0.4â”‚0.1â”‚0.7â”‚               â”‚0.8â”‚ 0 â”‚ 0 â”‚0.7â”‚                        â”‚
â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜               â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                        â”‚
â”‚                                                                             â”‚
â”‚   Still 4Ã—4 matrix! GPU processes same number of elements.                â”‚
â”‚   Speedup requires sparse matrix libraries.                                â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ STRUCTURED (entire columns/neurons):                                       â”‚
â”‚                                                                             â”‚
â”‚   Original matrix (4Ã—4):          After removing 2 neurons:                â”‚
â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”               â”Œâ”€â”€â”€â”¬â”€â”€â”€â”                                â”‚
â”‚   â”‚0.5â”‚0.3â”‚0.8â”‚0.2â”‚               â”‚0.5â”‚0.8â”‚                                â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤                                â”‚
â”‚   â”‚0.1â”‚0.7â”‚0.4â”‚0.9â”‚       â†’       â”‚0.1â”‚0.4â”‚    Matrix is now 4Ã—2!         â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤                                â”‚
â”‚   â”‚0.6â”‚0.2â”‚0.5â”‚0.3â”‚               â”‚0.6â”‚0.5â”‚    50% fewer multiplications  â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¤                                â”‚
â”‚   â”‚0.8â”‚0.4â”‚0.1â”‚0.7â”‚               â”‚0.8â”‚0.1â”‚    Works on ANY hardware!      â”‚
â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜               â””â”€â”€â”€â”´â”€â”€â”€â”˜                                â”‚
â”‚                                                                             â”‚
â”‚ TRADEOFF:                                                                   â”‚
â”‚   Unstructured: Higher compression possible, needs sparse libraries        â”‚
â”‚   Structured:   Lower compression, but real speedup on all hardware        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 7: Fine-Tuning After Pruning

After pruning, the model's accuracy drops. Fine-tuning allows remaining weights to compensate.

```python
def fine_tune_after_pruning(
    model: nn.Module,
    train_loader,
    val_loader,
    config,
    device: str
) -> nn.Module:
    """
    Fine-tune the model after pruning to recover accuracy.
    
    WHY FINE-TUNE?
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Pruning removes weights that were contributing (even if small).
    The remaining weights need to adjust to compensate.
    
    Without fine-tuning: 70% F1 â†’ 60% F1 (10% drop)
    With fine-tuning:    70% F1 â†’ 67% F1 (3% drop)
    
    Fine-tuning is CRITICAL for good pruning results!
    """
    print(f"\nğŸ”§ Fine-tuning pruned model for {config.fine_tune_epochs} epochs...")
    
    model.train()
    
    # Lower learning rate than initial training
    # We're fine-tuning, not training from scratch
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr * 0.1,  # 10% of original LR
        weight_decay=config.weight_decay
    )
    
    loss_fn = nn.BCEWithLogitsLoss()
    
    best_f1 = 0
    
    for epoch in range(config.fine_tune_epochs):
        total_loss = 0
        
        for batch in tqdm(train_loader, desc=f"Fine-tune Epoch {epoch+1}"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            optimizer.zero_grad()
            
            outputs = model(input_ids, attention_mask)
            logits = outputs['logits'] if isinstance(outputs, dict) else outputs
            
            loss = loss_fn(logits, labels)
            loss.backward()
            
            # IMPORTANT: Zero out gradients for pruned weights
            # This keeps pruned weights at zero!
            _zero_pruned_gradients(model)
            
            optimizer.step()
            total_loss += loss.item()
        
        # Evaluate
        f1 = evaluate_model(model, val_loader, device)
        print(f"   Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, F1={f1:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
    
    print(f"   âœ“ Fine-tuning complete. Best F1: {best_f1:.4f}")
    return model


def _zero_pruned_gradients(model: nn.Module):
    """
    Zero out gradients for pruned (zero) weights.
    
    WHY?
    Without this, pruned weights could become non-zero again during training!
    
    gradient Ã— lr = weight_update
    If pruned_weight = 0 but gradient â‰  0:
        new_weight = 0 + gradient Ã— lr â‰  0  â† Weight comes back!
    
    By zeroing gradients for pruned weights:
        new_weight = 0 + 0 Ã— lr = 0  â† Weight stays pruned!
    """
    for name, param in model.named_parameters():
        if param.grad is not None:
            # Create mask of non-zero weights
            mask = (param.data != 0).float()
            # Zero gradient where weight is zero
            param.grad.data *= mask
```

### Visualization of Fine-Tuning:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ WHY FINE-TUNING HELPS                                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Before pruning:                                                            â”‚
â”‚                                                                             â”‚
â”‚   Input â†’ [w1=0.5] â†’ [w2=0.1] â†’ [w3=0.8] â†’ Output                         â”‚
â”‚                â†˜     â†—                                                      â”‚
â”‚                [w4=0.05]                                                    â”‚
â”‚                                                                             â”‚
â”‚   All weights work together to produce correct output.                     â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ After pruning w4 (smallest):                                               â”‚
â”‚                                                                             â”‚
â”‚   Input â†’ [w1=0.5] â†’ [w2=0.1] â†’ [w3=0.8] â†’ Output (slightly wrong!)       â”‚
â”‚                â†˜     â†—                                                      â”‚
â”‚                [w4=0] â† pruned                                             â”‚
â”‚                                                                             â”‚
â”‚   w4's contribution is missing. Output is less accurate.                   â”‚
â”‚                                                                             â”‚
â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚
â”‚                                                                             â”‚
â”‚ After fine-tuning:                                                         â”‚
â”‚                                                                             â”‚
â”‚   Input â†’ [w1=0.52] â†’ [w2=0.12] â†’ [w3=0.82] â†’ Output (mostly correct!)    â”‚
â”‚                 â†˜     â†—                                                     â”‚
â”‚                 [w4=0] â† stays pruned                                      â”‚
â”‚                                                                             â”‚
â”‚   Remaining weights adjusted to compensate for w4's absence.              â”‚
â”‚   w1: 0.5 â†’ 0.52 (+4%)                                                     â”‚
â”‚   w2: 0.1 â†’ 0.12 (+20%)                                                    â”‚
â”‚   w3: 0.8 â†’ 0.82 (+2.5%)                                                   â”‚
â”‚                                                                             â”‚
â”‚   These small adjustments recover most of the lost accuracy!               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Section 8: Sparsity Tracking

```python
def get_sparsity(self) -> Dict[str, float]:
    """
    Calculate current sparsity statistics.
    
    Returns dict with:
        - overall: Total sparsity across all prunable layers
        - per_layer: Sparsity for each layer
        - zero_params: Count of zero parameters
        - nonzero_params: Count of non-zero parameters
    """
    total_params = 0
    zero_params = 0
    per_layer = {}
    
    for name, module in self.model.named_modules():
        if not isinstance(module, nn.Linear):
            continue
        
        weight = module.weight.data
        layer_total = weight.numel()
        layer_zeros = (weight == 0).sum().item()
        
        total_params += layer_total
        zero_params += layer_zeros
        
        per_layer[name] = layer_zeros / layer_total
    
    overall_sparsity = zero_params / total_params if total_params > 0 else 0
    
    return {
        'overall': overall_sparsity,
        'per_layer': per_layer,
        'zero_params': zero_params,
        'nonzero_params': total_params - zero_params,
        'total_params': total_params
    }
```

### Making Pruning Permanent:

```python
def make_pruning_permanent(self):
    """
    Convert pruning masks to actual zeros.
    
    PyTorch pruning uses masks: weight = weight_orig Ã— mask
    This removes the mask and bakes zeros into the weight.
    
    WHEN TO DO THIS:
        - After all pruning is complete
        - Before saving the model
        - Before quantization (quantization doesn't understand masks)
    """
    for module, name in self.prunable_layers:
        if prune.is_pruned(module):
            prune.remove(module, name)
    
    print("   âœ“ Pruning made permanent")
```

---

## Section 9: Utility Function to Get Any Pruner

```python
def get_pruner(
    model: nn.Module,
    method: str,
    target_sparsity: float,
    **kwargs
) -> Union[PruningManager, GradualPruner, WandaPruner, StructuredPruner]:
    """
    Factory function to get the right pruner based on method name.
    
    This is what research_main.py calls to get a pruner.
    """
    if method == 'magnitude':
        return PruningManager(model, target_sparsity, **kwargs)
    
    elif method == 'gradual':
        return GradualPruner(model, target_sparsity, **kwargs)
    
    elif method == 'wanda':
        return WandaPruner(model, target_sparsity, **kwargs)
    
    elif method == 'structured':
        return StructuredPruner(model, target_sparsity, **kwargs)
    
    else:
        raise ValueError(f"Unknown pruning method: {method}")
```

---

## Summary: Complete Pruning Method Comparison

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRUNING METHOD COMPARISON                                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ Method      â”‚ Complexity â”‚ Accuracy â”‚ Speed    â”‚ When to Use               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ magnitude   â”‚ Simple     â”‚ Good     â”‚ Fast     â”‚ Quick experiments         â”‚
â”‚             â”‚            â”‚          â”‚          â”‚ Baseline comparison       â”‚
â”‚             â”‚            â”‚          â”‚          â”‚                           â”‚
â”‚ gradual     â”‚ Medium     â”‚ Better   â”‚ Slower   â”‚ Training time available   â”‚
â”‚             â”‚ (needs     â”‚ (+2-5%)  â”‚          â”‚ Want best accuracy        â”‚
â”‚             â”‚ training)  â”‚          â”‚          â”‚                           â”‚
â”‚             â”‚            â”‚          â”‚          â”‚                           â”‚
â”‚ wanda       â”‚ Medium     â”‚ Best     â”‚ Medium   â”‚ State-of-the-art results  â”‚
â”‚             â”‚ (needs     â”‚ (+3-7%)  â”‚          â”‚ Paper submissions         â”‚
â”‚             â”‚ calibration)â”‚         â”‚          â”‚                           â”‚
â”‚             â”‚            â”‚          â”‚          â”‚                           â”‚
â”‚ structured  â”‚ Complex    â”‚ Moderate â”‚ Real     â”‚ Need actual speedup       â”‚
â”‚             â”‚            â”‚ (-2-5%)  â”‚ speedup! â”‚ Deployment constraints    â”‚
â”‚                                                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚ RECOMMENDED WORKFLOW:                                                       â”‚
â”‚                                                                             â”‚
â”‚ 1. Start with 'magnitude' at 50% for quick experiments                     â”‚
â”‚ 2. Try 'wanda' for better accuracy (if you have calibration data)          â”‚
â”‚ 3. Use 'gradual' if you have time for full training                        â”‚
â”‚ 4. Use 'structured' only if you need real speedup without sparse libs      â”‚
â”‚                                                                             â”‚
â”‚ Always fine-tune after pruning!                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## What You Can Modify for Research

| Category | What to Modify | Research Question |
|----------|----------------|-------------------|
| **Sparsity** | 0.3, 0.5, 0.7 | Compression-accuracy tradeoff |
| **Method** | magnitude, wanda, gradual | Which method works best? |
| **Schedule** | linear, cubic, exponential | Best gradual pruning schedule? |
| **Layers** | all, attention, ffn | Which layers are most compressible? |
| **Global** | True/False | Global vs per-layer pruning? |
| **Fine-tune** | epochs, learning rate | How much recovery is possible? |

---

## Experiments You Can Run

```bash
# Experiment 1: Sparsity sweep
for sparsity in 0.3 0.4 0.5 0.6 0.7; do
    python research_main.py --pipeline prune_only \
        --prune_method magnitude \
        --prune_sparsity $sparsity \
        --output_dir results/sparsity_$sparsity
done

# Experiment 2: Method comparison at 50% sparsity
for method in magnitude wanda gradual; do
    python research_main.py --pipeline prune_only \
        --prune_method $method \
        --prune_sparsity 0.5 \
        --output_dir results/method_$method
done

# Experiment 3: Layer-specific pruning
for layer in all attention ffn encoder; do
    python research_main.py --pipeline prune_only \
        --prune_method magnitude \
        --prune_sparsity 0.5 \
        --prune_layers $layer \
        --output_dir results/layer_$layer
done

# Experiment 4: Fine-tuning epochs
for epochs in 1 3 5 10; do
    python research_main.py --pipeline prune_only \
        --prune_method magnitude \
        --prune_sparsity 0.5 \
        --fine_tune_after_prune \
        --fine_tune_epochs $epochs \
        --output_dir results/finetune_$epochs
done
```

---

## Practice Exercise

Before moving to the next script:

1. **Calculate incremental sparsity**: If current is 40% and target is 60%, what fraction of remaining weights should be pruned?
2. **Compare methods mentally**: Why would Wanda keep a weight that magnitude pruning removes?
3. **Think about trade-offs**: When would you prefer structured over unstructured pruning?

---

**Ready for the next script? The next one is `research_quantization.py` which implements all quantization methods (dynamic, static, QAT, FP16, INT4).**

Would you like me to continue?